

\minitoc % Ajoute le sommaire local ici


% ==================================================================================================================================
% Introduction 

Précédement, nous avons vu que les langages automatiques sont de très bonnes propriétés. Ils sont stables 
pour la plupart des opérations définies sur les langages. De plus, le théorème de Kleene nous a permis d'établir 
le lien direct entre langages automatiques et langages réguliers. 
La simplicité de la représentation sagitale des automates permet de les implémenter facilement algorithmiquement. 
Il sont, de plus, facile à manipuler à la main et permettent de rapidement "voir" les langages reconnus. 

Cependant, cette simplicité a un certain coût, celui de ne pas pouvoir reconnaître des langages "compliqués", notamment
ceux où il faut "compter" les lettres. Un automate fini déterministe ne peut donc pas reconnaître le langage composé 
d'autant de $a$ que de $b$. 
On va donc chercher à indroduire une nouvelle théorie, celle des \textbf{grammaires formelles} qui nous permettra
de reconnaître de tels langages. 


% ==================================================================================================================================
% Grammaires Formelles

\section{Grammaires Formelles}

\subsection{Contexte et définition}

Les grammaires formelles ont initialement été développés par des linguistes, notamment Noam Chomsky en 1955. 
L'objectif était de développer un méthode systématique de traduction entre différentes langues. 
Ils se sont alors heurtés au problème des mêmes mots qui admettent plusieurs traductions en fonction du contexte de la 
phrase et n'ont pas pu aboutir leur oeuvre. 

\vspace{0.3cm}

Or en informatique, pour l'étude de la syntaxe de langages de programmation, le problème du contexte ne se pose pas. 
Leur théorie a donc été récupérée pour la vérification syntaxique. 

\vspace{0.3cm}

L'idée est donc de représenter un langage \textbf{récursivement} par un ensemble de règles de production composées 
d'un axiome de départ et de différentes règles de productions ou de réécriture. 

Nous utilisons souvent cette approche pour la gestion de types en Caml en définissant tes types récurisivement. 


\begin{definition}[Grammaire Formelle]
    Une grammaire formelle est un quadruplet 
        \[ G = (\Sigma, V, S, P) \] 
    où 
    \begin{itemize}
        \item $\Sigma$ est un \textbf{alphabet terminal} dont chaque élément ne peut se réécrire plus simplement. 
        \item $V$ est \textbf{l'alphabet auxiliaire} (disjoint de $\Sigma$) composé de variables, qui ne 
        peuvent pas non plus se réécrire. 
        \item $S$ est la variable de départ, appelé axiome. 
        \item $P$ est un ensemble de règles dites \textbf{de production} ou de réécriture du type 
            \[ X \longrightarrow w \quad X \in V \text{ et } w \in \left( V \cup \Sigma \right)^* \] 
    \end{itemize}
\end{definition}

Par convention, on notera toujours les variables en majuscule et les éléments terminaux en minuscules. 
En pratique, on regroupera plusieurs réécritures d'une même variable sur la même ligne en les séparant par 
des barres verticales de la forme :
    \[ X \longrightarrow w_1 | w_2 | \dots | w_p \iff 
        \begin{cases}
            X \longrightarrow w_1 \\ 
            X \longrightarrow w_2 \\ 
            \vdots \\ 
            W \longrightarrow w_p 
        \end{cases}
    \] 


\subsection{Réécriture d'un mot et langages algébriques}

L'obectif d'une grammaire formelle, vous l'aure compris, est de réécrire un mot récursivement jusqu'à arriver à des 
éléments terminaux. 

\begin{definition}[Réécriture d'un mot]
    Soit $ G = (\Sigma, V, S, P)$ une grammaire formelle. Soient $u,v \in \left( V \cup \Sigma \right)^*$ deux mots. 
    On dit que \textbf{$u$ peut se réécrire en $v$ en une étape} et on note :
        \[ u \vdash v \] 
    si il existe des décompositions de $u$ et $v$ en 
        \[ u = u_1 X u_2 \text{ et } v = u_1 w u_2 \] 
    et que $G$ contient la règle de production :
        \[ X \longrightarrow w \] 
\end{definition}

Plus généralement, on peut définir la réécriture en plusieurs étapes de la forme : 

\begin{definition}[Réécriture (2)]
    Soit $ G = (\Sigma, V, S, P)$ une grammaire formelle. Soient $u,v \in \left( V \cup \Sigma \right)^*$ deux mots.
    On dit que \emph{$u$ peut se réécrire en $v$} ou que \emph{$v$ dérive en $u$} en un nombre quelconque de fois si 
    il existe $u_1, \dots, u_p \in \left( V \cup \Sigma \right)^*$ tels que 
        \[ u \vdash u_1 \vdash u_2 \vdash \dots \vdash u_p \vdash v \] 
    On note alors 
        \[ u \overset{*}{\vdash} v \] 
\end{definition}

On peut maintetant définir les langages engendrés par des grammaires formelles et les langages algébriques, le coeur de ce 
chapitre. 

\begin{definition}[Langage Engendré]
    La \emph{langage engendré} par une grammaire formelle $ G = (\Sigma, V, S, P)$ est l'ensemble des mots de $\Sigma^*$ 
    qui dérivent de l'axiome $S$ en un nombre quelconque d'étapes. On le note, comme pour les automates, $L(G)$. 
\end{definition}

\begin{definition}[Langage Algébrique]
    Un langage engendré par une grammaire est appelé \emph{langage algébrique}. 
\end{definition}



\subsection{Arbre de dérivation d'un mot}

\begin{definition}[Arbre de dérivation d'un mot]
    Soit $ G = (\Sigma, V, S, P)$ une grammaire formelle. On appelle l'arbre de dérivation de $w \in Sigma^*$ l'arbre dont :
    \begin{itemize}
        \item La racine est $S$ 
        \item Tous les sommets intérieurs appartiennent à $V$ 
        \item Toutes les feuilles appartiennent à $\Sigma \cup \{\varepsilon\}$ 
        \item Si un sommet intérieur $X$ a pour fils $X_1, \dots, X_p$ alors la règle 
            \[ X \longrightarrow X_1 | \dots | X_p \in P \] 
        \item Le mot obtenu en visitant les feuilles de l'arbre par un parcours profondeur préfixe de l'arbre 
            est un mot de $L(G)$
    \end{itemize}
\end{definition}

\begin{definition}[Grammaire Ambiguë]
    Soit $G$ une grammaire. On dit que $G$ est ambiguë s'il existe un mot $w_ \in L(G)$ possédant
    deux arbres de dérivation différents. 
\end{definition}

En pratique une telle grammaire est pas très utilisée. En effet, en informatique, il ne serait pas très pratique 
de pouvoir compiler un code en deux expressions différentes d'un autre langage. On ne saurait pas laquelle choisir. 
Il faut que la dérivation puisse se faire de façon unique.

\begin{example}[Grammaire Formelle et Arbre de dérivation]
    Soit $ \Sigma = \{a,b\}$ . 
    Soit la grammaire formelle $G$ définie par les règles suivantes telles que $ V = \{S\}$ : 
        \[  P : 
            \begin{cases}
                S \longrightarrow aSa \\ 
                S \longrightarrow SbS \\ 
                S \longrightarrow \varepsilon
            \end{cases}
            \iff 
            \begin{cases}
                S \longrightarrow aSa \; | \; bSb \; | \;  \varepsilon
            \end{cases}
        \] 
    Soit $abaaba \in G$ on a alors l'arbre de dérivation suivant pour ce mot : 
    \begin{center}
        \begin{forest}
            [S
                [a]
                [S 
                    [b]
                    [S 
                        [a]
                        [S [$\varepsilon$]]
                        [a]
                    ]
                    [b]
                ]
                [a]
            ]
            \end{forest}
    \end{center}
    Cette grammaire reconnaît bien les palindrômes pairs. 
\end{example}

\begin{remark}
    Lors de la dérivations de mots par une grammaire, on remarque qu'il est plus facile que les règles de dérivation 
    possèdent des traces initiales ou finales uniques telles que les $a$ et les $b$. 
    Elles permettent d'identifier plus facilement les règles à utiliser pour les dérivations. 
\end{remark}



\subsection{Grammaires Régulières}

Nous allons ici faire le lien entre les deux modèles présentés précedement, les automates fini et les grammaires formelles. 
Nous allons ainsi définir les grammaires régulières qui permettent de représenter les automates fini déterministes sous 
la forme que nous venons d'introduire. 

\begin{prop}[Représentation d'un langage automatique]
    Soit $ \mathcal{A} = (Q, \Sigma, T, q_0, A)$ un automate fini déterministe. 
    Le langage $L$ reconnu par cet automate peur être engendré par la grammaire : 
        \[ G = (Q, \Sigma, q_0, P) \] 
    dont les variables auxiliaires sont les états de l'automate et où $P$ est l'ensemble 
    des productions de la forme :
        \[ q \longrightarrow x. T(q,x) \quad \text{où } q \in Q \text{ et } x \in \Sigma \] 
        \[ q \longrightarrow \varepsilon \quad \text{ si } q \in A \] 
\end{prop}

On peut donc représenter facilement n'importe quel langage automatique par une grammaire formelle. 
D'où le théorème suivant. 

\begin{theorem}[Langage Automatique et Grammaire Formelle]
    Tout langage automatique (reconnaissable par un automate fini) est algébrique (reconnaissable par une grammaire formelle). 
    
    \vspace{0.2cm}

    L'ensemble des langages automatiques est même strictement inclus dans l'ensemble des langages algébriques. 
    Autrement dit, certains langages sont reconnaissables par une grammaire formelle mais pas par un automate. 
\end{theorem}

On définit ainsi les grammaires régulières. 

\begin{definition}[Grammaire Régulière]
    Une grammaire régulière est une grammaire formelle dont toutes les règles de production de $P$ sont de la forme :
        \[ X \longrightarrow a.Y \quad \text{ou } X \longrightarrow \varepsilon \]
    où $X,Y \in V$ et $a \in \Sigma$.  
\end{definition}

Une grammaire régulière est donc conçue de façon à "laisser des traces" explicites de la structure des 
mots pour faciliter les dérivations. De même que précédement, on peut passer d'une grammaire régulière à un automate 
fini déterministe. 

\begin{proposition}[Représentation d'une grammaire régulière]
    Soit $G$ une grammaire régulière. Soit $L$ le langage reconnu par $G$.
    L'automate fini déterministe reconnaissante aussi $L$ est : 
        \[ \mathcal{A} = (V,\Sigma, T, q_0 = S, A) \] 
    dont les états sont les variables auxiliaires de $G$ et dont les transitions sont définies par : 
        \[ q' = T(q,x) \quad \text{si} \quad q \longrightarrow x q' \in P \] 
    et dont les états acceptants sont définis par :
        \[ q \in A \quad \text{si} \quad q \longrightarrow \varepsilon \in P \]  
\end{proposition}

\begin{remark}
    Grâce à cette propriété, les langages automatiques (réguliers) sont donc exactement les langages reconnus par des grammaires régulières. 
    D'où le nom...
\end{remark}

\begin{example}[Construction d'une grammaire régulière]
    Définissons le langage $L$ reconnaissant les mots contenant un nombre pair de $a$ et impair de $b$. 
    Alors ce langage est reconnu par l'AFn suivant : 
    \begin{center}
        \begin{tikzpicture}[shorten >=1pt, node distance=3cm, on grid, auto]
            \node[state, initial] (1) {$1$};
            \node[state, accepting, right of=1] (2) {$2$};
            \node[state, below of=1] (3) {$3$};
            \node[state, below of=2] (4) {$4$};

            \path[->]
            (1) edge[above, bend right] node{$b$} (2)
            (2) edge[above, bend right] node{$b$} (1)
            (2) edge[right, bend right] node{$a$} (4) 
            (4) edge[right, bend right] node{$a$} (2) 
            (1) edge[right, bend right] node{$a$} (3)
            (3) edge[right, bend right] node{$a$} (1)
            (3) edge[above, bend right] node{$b$} (4)
            (4) edge[above, bend right] node{$b$} (3);
        \end{tikzpicture}
    \end{center}
    
    D'après la propriété précédente, $L$ est reconnu par la grammaire régulière $G = (\Sigma, V, S, P)$
    où $V = \{S,A,B,C\}$ et les états sont représentés par :
        \[ 
            \begin{cases}
                1 \longrightarrow S \\ 
                2 \longrightarrow A \\ 
                3 \longrightarrow B \\ 
                4 \longrightarrow C
            \end{cases} \] 
    On peut ensuite déterminer les règles de production à partir du voisinnage sortant 
    de chaque état de l'automate. De plus, puisque $A$ est un état acceptant, on y rajouter $\varepsilon$. 
    \[ P : 
            \begin{cases}
                S \longrightarrow bA \; | \; aB \\ 
                A \longrightarrow bS \; | \; aC \; | \; \varepsilon \\ 
                B \longrightarrow aS \; | \; cB \\ 
                C \longrightarrow aA \; | \; bB 
            \end{cases} \] 
\end{example}


% ==================================================================================================================================
% Forme Normale de Chomsky 

\section{Forme Normale de Chomsky}

Tout comme les automates, on va chercher à simplifier les grammaires. 
Or ici, pour un langage donné il n'existe pas de forme minimale de grammaire qui l'engendre.

On va donc chercher à simplifier les grammaires dans le but d'obtenir des formes dites normales 
pour réduire le nombre de dérivations à faire pour un mot donné. L'idée est de ramener les arbres de dérivation 
à des arbres binaires donc ekes dérivations sont seulement de deux formes. 
On pourra donc calculer directement la profondeur de l'arbre de dérivation de n'importe quel mot du langage
engendré en fonction de son nombre de caractères. 

Pour cela, il faut définir un certain nombre de règles qui serviront à cette simplification. 
Commençons par un règle très simple : 

\begin{definition}[Règle 0]
    On peut toujours éliminer une règle de la forme $ X \longrightarrow X$. 
\end{definition}

\subsection{Règle 1 : Suppression des epsilon-productions}

On va chercher ici à supprimer toutes les $\varepsilon$ productions qui ne produisent rien dans la 
dérivation d'un mot et prennent beaucoup de temps et de place à exécuter. 

\begin{definition}[Règle 1 : Suppression des espilon-productions]
    Soit $G$ une grammaire formelle. On définit l'algorithme suivant pour 
    supprimer toutes les $\varepsilon$-productions de $ G$ en une grammaire équivalente : 
    \begin{enumerate}
        \item On cherche récursivement toutes les variables dont $\varepsilon$ dérive
        (i.e toutes les variables qui peuvent nous donner $\varepsilon$ à la fin). 
        \item On supprimer toutes les règles de la forme $X \longrightarrow \varepsilon$. 
        \item Pour toutes les variables $X$ de la forme $X \longrightarrow w$ 
        on rajoute toutes les productions $X \longrightarrow u$ avec $ u \not = u$ et $u$ est 
        obtenu à partir de $w$ en remplaçant une ou plusieurs variables indentifiées en 1. 
    \end{enumerate}
\end{definition}

\begin{example}
    Soit $G$ une grammaire d'alphabet $\Sigma = \{a,b\}$ et $V = \{S,A,B\}$ tel que :
        \[ P : 
            \begin{cases}
                S \longrightarrow AB | aS | A \\ 
                A \longrightarrow Ab | \varepsilon \\ 
                B \longrightarrow B | AS 
            \end{cases}
        \] 
    On cherche $G'$ telle sans $\varepsilon$-productions telle que :
        \[ L(G') \cup \{\varepsilon\} = L(G) \] 
    D'après la règle 1, toutes les variables produisent une $\varepsilon$-production. 
    On obtient donc la grammaire équivalente à $\varepsilon$-production près :
        \[ P' : 
            \begin{cases}
                S \longrightarrow AB | A | B | aS | a \\ 
                A \longrightarrow Ab | b \\ 
                B \longrightarrow AS | A | S 
            \end{cases} \] 
\end{example}

\subsection{Règle 2 : Élimination des cycles}

Dans les arbres de dérivation, les cycles peuveut conduire à des dérivations infinies. 
On va donc chercher à les supprimer. 

\newpage 

\begin{definition}[Règle 2 : Élimination des cycles]
    Soit $G$ une grammaire formelle. On définit l'algorithme suivant pour 
    supprimer tous les cycles de $ G$ en une grammaire équivalente. 
    Soit un cycle de la forme : 
        \[ X_1 \longrightarrow X_{n-1} \longrightarrow X_1 \] 
    Alors on remplace dans $P$ toutes les variables $X_i \forall i \in \llbracket 1, n-1 \rrbracket$ 
    par $X_1$. 
\end{definition}

\begin{example}
    En reprennant l'exemple précédent : 
    \[ P' : 
            \begin{cases}
                S \longrightarrow AB | A | B | aS | a \\ 
                A \longrightarrow Ab | b \\ 
                B \longrightarrow AS | A | S 
            \end{cases} \] 
    On détecte un seul cycle : $ S \longrightarrow B \longrightarrow S$. 
    On applique donc l'algorithme pour obtenir : 
    \[ P'' : 
            \begin{cases}
                S \longrightarrow AS | A | aS | a \\ 
                A \longrightarrow Ab | b \\ 
            \end{cases} \] 
\end{example}

\subsection{Règle 3 : Suppression des changements de variable}

Les changement de variable dans les arbres de dérivation font perdre du temps. 
En effet, ils augmentent la profondeur de l'arbre de dérivation sans produire de lettre. 
On va donc chercher à les supprimer avec la règle 3. 

\begin{definition}[Règle 3 : Suppression des changements de variable]
    Soit $G$ une grammaire formelle. Soit une dérivation de la forme $ A \longrightarrow B \longrightarrow C$. 
    Alors on peut la remplacer en $A \longrightarrow C$. 
\end{definition}

\begin{example}
    En reprenant l'exemple précédent, on peut supprimer les changements de variable : 
    \[ P'' : 
            \begin{cases}
                S \longrightarrow AS | A | aS | a \\ 
                A \longrightarrow Ab | b \\ 
            \end{cases} \] 
    On a donc les règles de productions suivantes en remplaçant :
    \[ P''' : 
            \begin{cases}
                S \longrightarrow AS | Ab | b | aS | a \\ 
                A \longrightarrow Ab | b 
            \end{cases} \] 
\end{example}


\subsection{Forme Normale de Chomsky}

Pour l'instant, on ne peut pas encore déterminer la profondeur de l'arbre de dérivation d'un mot. 
Il faut donc définir une forme, dite Normale de Chomsky, qui va permettre cette estimation. 

\begin{definition}[Forme Normale de Chomsky]
    Soit $ G = (\Sigma, V, S, P)$ une grammaire formelle. 
    Supposons que $G$ ne contienne ni $\varepsilon$-production, si changements de variables, 
    ni cycles. On définit alors la forme normale de Chomsky de ses règles de production $P$ comme ses 
    mêmes règles de production où seules deux formes sont autorisées : 
    \begin{itemize}
        \item Les productions de lettres de la forme $ X \longrightarrow a$ 
        \item Les dédoublements de variables de la forme $ X \longrightarrow AB$ 
    \end{itemize}
\end{definition}

\begin{example}
    En reprenant l'exemplement précédent, on obtient la forme normale de Chomsky suivante : 
    \[ \overset{\sim}{P} : 
        \begin{cases}
            S \longrightarrow AS | AY | b | XS | a \\ 
            A \longrightarrow AY | b \\ 
            X \longrightarrow a \\ 
            Y \longrightarrow b 
        \end{cases} \] 
\end{example}

On en déduit donc le théorème suivant :

\begin{theorem}[Majoration de la profondeur]
    Soit $w \in L(G)$ où $G$ est une grammaire formelle sous forme normale de Chomsky. 
    Notons $p$ la profondeur de l'arbre de dérivation de $w$ dans cette grammaire $g$. 
    On a alors l'inégalité suivante : 
        \[ \boxed{p \leqslant 2 |w| -1 } \] 
\end{theorem}

% ==================================================================================================================================
% Analyse Synatxique (Grammaires LL(k))

\section{Analyse Syntaxique et approche descendante (Grammaires $LL(k)$)}

Lors de la dérivation d'un mot par une grammaire, on peut se heurter à un problème de taille : quel règle de dérivation choisir 
pour obtenir un arbre de dérivation acceptant ? 
Nous allons donc introduire un type de grammaire nous permettant de régler ce problème. Un seul type de dérivation 
sera possible et nous saurons exactement combien de caractères regarder dans notre mot pour choisir la règle 
de dérivation à appliquer. 

Les grammaires $LL(k)$ vont nous permettre de faire directement le lien avec le processus de \emph{parsing}, indispensable 
en compilation. 

\begin{definition}[Grammaire $LL(k)$]
    Soit $k \in \N$ et $G$ une grammaire. On dit que $G$ est $LL(k)$ si on peut analyser n’importe quel mot 
    de son langage de gauche à droite, en construisant la dérivation la plus à gauche, et en regardant au plus $k$ 
    symboles d’entrée à la fois, pour décider de manière univoque quelle production appliquer à chaque étape.
\end{definition}

\begin{remark}
    Détaillons un peu cette définition. La grammaire $G$ est $LL(k)$ si elle satsifait trois critères 
    principaux : 
    \begin{itemize}
        \item \textbf{L (Left to right)} : Lorsque l'on dérive un mot par la grammaire, on veut pouvoir 
        le lire de gauche à droite en commençant par le premier symbole. 
        \item \textbf{L (Leftmost derivation)} : Lorsque l'on remplace une variable non terminale 
        de l'espression en cours de dérivation, on veut que ce soit celle la plus à gauche. 
        \item \textbf{k (k lookahead)} : $k$ est le nombre de symbole que le parser peut regarder 
        pour déterminer quelle règle utiliser sans les consommer. 
    \end{itemize}
\end{remark}

Intuitivement, on cherche une grammaire prédictive. Pour chaque variable à chaque étape de la dérivation, 
le parser doit savoir exactement quelle règle appliquer en regardant $k$ lettres. Évidement, on ne considèrera 
pas de grammaire ambiguë. Nous allons donc développer un algorithme pour construire une \emph{table de dérivation} 
qui nous donnera la règle à appliquer à chaque étape de l'analyse syntaxique. 

\begin{definition}[$k$ - Lookadhead]
    Soit $G = ( \Sigma, V, S, P)$ une grammaire $LL(k)$ et une règle de production $A \rightarrow \alpha$. On définit 
    l'ensemble $LA(k, A \rightarrow \alpha)$ comme l'ensemble des chaînes de longueur $ \leqslant k$ qui déclenchent 
    la règle de production $A \rightarrow \alpha$ tel que : 
        \[ \boxed{LA(k, A \rightarrow \alpha) := \{w \in \Sigma^{ \leqslant k} | w \text{ peut apparaître si on choisit la règle } A \rightarrow \alpha\}} \] 
\end{definition}

Intuitivement, $LA(k, A \rightarrow \alpha)$ est l'ensemble des lettres que doit regarder le parser pour déterminer 
quelle règle de production appliquer. 

\begin{definition}[Ensemble First]
    Soit $G = ( \Sigma, V, S, P)$ une grammaire $LL(k)$. 
    Soit $X \in V$ un état \emph{non terminal}. On définit l'ensemble $First(X)$ comme l'ensemble 
    des \emph{élément terminaux de $ \Sigma$} qui peuvent apparaître en premier lors d'une dérivation de $X$. 
    De plus, si $X$ peut produire un $ \varepsilon$, alors $ \varepsilon \in  First(X)$. 
\end{definition}

\begin{prop}[Règles de la calcul (First)]
    Soit $G = ( \Sigma, V, S, P)$ une grammaire $LL(k)$, on a les règles de calcul suivantes : 
    \begin{itemize}
        \item Soit $a \in \Sigma$ un terminal, on a : $ First(a) = \{a\}$ 
        \item $First( \varepsilon) = \{ \varepsilon\}$ 
        \item Soit $A \in V$ une variable avec les productions : $A \rightarrow \alpha_1 | \dots | \alpha_n$ on a :
            \[ First(A) = \bigcup_{i \in \llbracket 1, n \rrbracket} First( \alpha_i) \] 
    \end{itemize}
\end{prop}

\begin{definition}[Ensemble Follow]
    Soit $G = ( \Sigma, V, S, P)$ une grammaire $LL(k)$. 
    Soit $X \in V$ un état \emph{non terminal}. On définit $Follow(A)$ comme l'ensemble 
    de tous les éléments terminaux qui peut apparaître directement après $A$ dans une dérivation complète. 
    On l'utilise quand une production peut produire $ \varepsilon$. 
\end{definition}

% \begin{prop}[Règles de calcul (Follow)]
%     Soit $G = ( \Sigma, V, S, P)$ une grammaire $LL(k)$. 
%     \begin{itemize}
%         \item Soit $ \$ $ le symbole de départ et $S$ l'axiome, on a : $ \$ \in Follow(S)$. 
%         \item Pour chaque production $ A \rightarrow \alpha B \beta$ où $ \alpha, \beta \in \Sigma$, on a : 
            
%     \end{itemize}
% \end{prop}

\begin{theorem}[Calcul de $LA(k)$]
    Soit $G = ( \Sigma, V, S, P)$ une grammaire $LL(k)$ et une règle de production $A \rightarrow \alpha$.
    On a : 
        \[ \boxed{LA(k, A \rightarrow \alpha) = First_k( \alpha) \cup \{x \in Follow_k(A) | \alpha \text{ peut dériver } \varepsilon\}} \] 
    plus précisément : 
    \begin{itemize}
        \item Si $ \alpha$ ne peut pas dériver $ \varepsilon$ : $LA(k, A \rightarrow \alpha) = First_a( \alpha)$ 
        \item Si $ \alpha$ peut dériver $ \varepsilon$ : $ LA(k, A \rightarrow \alpha) = First_a( \alpha) \cup Follow_k(A) $
    \end{itemize}
\end{theorem}

L'ensemble de ces règles de calcul vont nous permettre de définir quelle règle de dérivation appliquer à chaque 
étape de l'analyse syntaxtique d'un mot. Nous regrouperons toutes ces règles dans une table appelée la 
\emph{table d'analyse}. 

\begin{definition}[Table d'analyse]
     Soit $G = ( \Sigma, V, S, P)$ une grammaire $LL(k)$. Pour chaque règle de 
     production $A \longrightarrow \alpha$ et chaque $w \in LA(k, A \longrightarrow \alpha)$, on définit : 
        \[ M[A, w] = \alpha \] 
    Autrement dit, $ M[A,w]$ contient la partie droite de la règle de production à utiliser quand le non-terminal 
    courant est $A$ et que les $k$ prochains symboles de l'entrée forment $w$. 
\end{definition}

La table d’analyse d’une grammaire $LL(k)$ permet, pour chaque non-terminal $A$ et chaque séquence de 
$k$ symboles de lookahead $w$, de déterminer quelle production appliquer.

\begin{example}
    Considérons la grammaire :
        \[
            \begin{aligned}
                S &\to A\,b \mid c \\
                A &\to d \mid \varepsilon
            \end{aligned}
        \]

    {Étape 1 : Ensembles FIRST et FOLLOW}
        \[
            \begin{array}{lcl}
                \text{FIRST}(A) &=& \{ d, \varepsilon \} \\[3pt]
                \text{FIRST}(A b) &=& \{ d, b \} \\[3pt]
                \text{FIRST}(S) &=& \{ d, b, c \} \\[6pt]
                \text{FOLLOW}(S) &=& \{ \$ \} \\[3pt]
                \text{FOLLOW}(A) &=& \{ b \}
            \end{array}
        \]

    {Étape 2 : Ensembles LA(1)}

        \[
            \begin{array}{|c|c|c|c|}
                \hline
                \textbf{Production} & \textbf{FIRST} & \textbf{FOLLOW (si $\varepsilon \in \text{FIRST}$)} & \textbf{LA(1)} \\
                \hline
                S \to A b & \{d, b\} & - & \{d, b\} \\
                \hline
                S \to c   & \{c\} & - & \{c\} \\
                \hline
                A \to d   & \{d\} & - & \{d\} \\
                \hline
                A \to \varepsilon & \{\varepsilon\} & \{b\} & \{b\} \\
                \hline
            \end{array}
        \]

   On peut alors écrire la table d'analyse de la grammaire : 

        \[ 
            \begin{array}{|c|c|c|c|c|}
                \hline 
                \textbf{Non Terminal} & \textbf{b} & \textbf{c} & \textbf{d} & \textbf{\$} \\ 
                \hline 
                {S} & S \rightarrow Ab & S \rightarrow c & S \rightarrow Ab & \text{(erreur)} \\ 
                \hline 
                {A} & A \rightarrow \varepsilon & \text{(erreur)} & A \rightarrow d & \text{(erreur)} \\ 
                \hline
            \end{array}
        \] 
\end{example}

\begin{example}
    Considérons la grammaire :
        
        \[
            \begin{aligned}
                S &\to aA \mid aB \\
                A &\to b \\
                B &\to c
            \end{aligned}
        \]

    Pour les deux productions de $S$, le premier symbole est $a$ :
        \[
            \text{FIRST}(aA) = \text{FIRST}(aB) = \{a\}
        \]

    Cette grammaire n'est donc {pas LL(1)} : avec un seul symbole de regard, on ne peut pas distinguer $aA$ de $aB$.

    {Calculons les ensembles LA(2)} : On regarde deux symboles $(k=2)$ pour décider :

        \[
            \begin{array}{|c|c|c|}
                \hline
                \textbf{Production} & \textbf{Chaînes possibles} & \textbf{LA(2)} \\
                \hline
                S \to aA & ab & \{ab\} \\
                \hline
                S \to aB & ac & \{ac\} \\
                \hline
                A \to b & b & \{b\} \\
                \hline
                B \to c & c & \{c\} \\
                \hline
            \end{array}
        \]

    {Conclusion :} Avec $k=2$, on distingue les deux productions de $S$ :
    \begin{itemize}
        \item[-] Si les deux prochains symboles sont \texttt{ab}, on choisit $S \to aA$.
        \item[-] Si les deux prochains symboles sont \texttt{ac}, on choisit $S \to aB$.
    \end{itemize}

        \[
            \begin{array}{|c|c|c|}
                \hline
                \textbf{Non-terminal} & \textbf{ab} & \textbf{ac} \\
                \hline
                S & S \to aA & S \to aB \\
                \hline
            \end{array}
        \]

        Pour les non-terminaux $A$ et $B$, un seul symbole de lookahead suffit :
        \[
            \begin{array}{|c|c|}
                \hline
                \textbf{Non-terminal} & \textbf{Symbole} \\
                \hline
                A & b \Rightarrow A \to b \\
                \hline
                B & c \Rightarrow B \to c \\
                \hline
            \end{array}
        \]
    La grammaire est donc {LL(2)}, mais pas LL(1).
\end{example}

Ces nouveaux modèles de grammaires, quoique puissants ne permettent pas d'effectuer l'analyse syntaxique 
de n'importe quel langage. En effet, toutes les grammaires ne sont pas $LL(k)$ nous devrions donc 
modifier celles qui ne le sont pas pour effectuer l'analyse. 

% ==================================================================================================================================
% Analyse Syntaxique et approche ascendante (Grammaires LR(k))

\section{Analyse Syntaxique et approche ascendante (Grammaires $LR(k)$)}

\subsection{Principe}

Une autre approche de l'analyse syntaxique grâce aux grammaires et d'utiliser l'approche \emph{descendante}.
En effet, depuis le début nous cherchons à dériver un mot à partir de l'axiome de départ en descendant 
vers des états terminaux. Nous allons ici chercher à faire le contraire : à partir d'un mot nous chercherons 
à le réduire vers l'axiome. C'est ce que l'on appelle l'approche \emph{ascendante}, aussi appelée 
\emph{analyse par décalage/réduction (shift-reduce)}. 

\begin{example}
    Commençons par étudier un exemple pour bien comprendre le principe de ces grammaires. 
    Soit la grammaire $G_1$ suivante : 
        \[ 
            \begin{cases}
                E \to E + E \\ 
                E \to E * E \\ 
                E \to (E) \\ 
                E \to id 
            \end{cases}
        \] 
    Soit $ w \in \Sigma^*$ tel que $w := id + id * id$. Nous cherchons à réduire $w$ grâce aux règles de 
    productions de $G_1$ jusqu'à l'axiome $S$. Résumons tout ça dans un tableau : 
        \[ 
            \begin{array}{|c|c|c|}
                \hline 
                \text{Proto-phrase de droite} & \text{Poignée (handle)} & \text{Production de réduction} \\ 
                \hline 
                id + id * id & id & E \to id \\
                \hline  
                E + id * id & id & E \to id \\ 
                \hline
                E + E * id & id & E \to id \\ 
                \hline 
                E + E * E & E * E & E \to E * E \\ 
                \hline
                E + E & E + E & E \to E + E \\ 
                \hline 
                E & & \\
                \hline 
            \end{array}
        \] 
\end{example}

\subsection{Outils Fondamentaux}

Définissons maintenant quelques outils pour effectuer rigoureusement cette analyse ascendante. 

\begin{definition}[proto-mot]
    Soit $G = (V, \Sigma, S, P)$ une grammaire. Un \emph{proto-mot} (ou \emph{chaîne sentencielle}) est 
    toute chaîne de symboles, terminaux et/ou non terminaux, que l'on peut obtenir en partant de 
    l'axiome et en effectuant un nombre quelconque de dérivations. 
    Formellement, pour la grammaire $G$, on a : 
        \[ \alpha \in (V \cup \Sigma)^* \text{ est un proto-mot si } S \to^* \alpha \] 
\end{definition}

Un proto-mot est simplement une "étape intermédiaire" de la dérivation d'un mot. Dans 
l'ensemble précédent, cela correspond à $ E + E * id$. Il est composé du terminal $id$ et 
du non terminal $E$. 

\begin{definition}[Poignée (Handle)]
    Soient $G = (V, \Sigma, S, P)$ une grammaire et $w$ un proto-mot de $G$. 
    Une \emph{poignée} de $w$ est occurence précise du corps d'une production $A \to \alpha$ dans 
    $w$ telle que : 
        \[ S \to^* \gamma A \delta \to \gamma \alpha \delta = w \]
\end{definition}

Une poignée correspond donc à une sous-chaîne à réduire dans notre mot $w$ à un instant donné de l'analyse ascendante. 
Dans notre exemple précédent, pour le proto-mot $ E + E * id$, une poignée peut correspondre au non terminal 
$id$ qui se réduit en $E$ en une étape grâce à la règle $ E \to id$. 

\begin{definition}[Préfixe Viable]
    Soient $G = (V, \Sigma, S, P)$ une grammaire et $w$ un proto-mot de $G$. 
    Une \emph{préfixe viable} est toute chaîne de symbole $ \alpha\in V^*$ telle que : 
        \[ \exists \beta \gamma \text{  tels que  }  S' \to^* \alpha \beta \gamma \text{ et } \beta \gamma \text{ contient une poignée complète} \] 
    où $S' \to S$ est l'axiome augmenté.   
\end{definition}

Un préfixe viable est exactement ce que l'on veut avoir au dessus de la pile d'un analyseur $LR$ si l'on ne veut 
pas de retrouver dans une impasse : 
\begin{itemize}
    \item Si la pile contient un préfixe viable, on est dans une configuration valide et l’analyse peut continuer.
    \item Si la pile contient quelque chose qui n’est pas un préfixe viable, alors l’analyseur détecte une erreur syntaxique.
\end{itemize}

\begin{definition}[Configuration Valide]
    Soient $G = (V, \Sigma, S, P)$ une grammaire. Une configuration d'un analyseur ascendant se décrit par : 
    \begin{itemize}
        \item Le \emph{contenu de la pile}
        \item Le \emph{reste de l'entrée} non consommé. 
    \end{itemize}
    Une configuration est dite \emph{valide} si la pile contient un \emph{préfixe viable} qui peut mener à 
    une dérivation complète de l'axiome. 
\end{definition}

Une configuration valide est une sorte d'instantanné de l'analyse où l'on n'a pas fait d'erreur de parsing. 

\subsection{Analyseur LR(k)}

Cette ensemble de définitions nous permet de définir formellement les grammaires $LR(k)$ de la 
façon suivante. 

\begin{definition}[Grammaire $LR(k)$]
    Soit \(G = (V, \Sigma, P, S)\) une grammaire, où :
    \begin{itemize}
        \item \(V\) est l'ensemble des symboles (terminaux et non-terminaux),
        \item \(\Sigma \subseteq V\) est l'ensemble des terminaux,
        \item \(P\) est l'ensemble des productions de la forme \(A \to \alpha\),
        \item \(S \in V \setminus \Sigma\) est l'axiome.
    \end{itemize}

    On dit que \(G\) est une \emph{grammaire LR(k)} si et seulement si,
    pour toute dérivation à droite :
    \[
    S \Rightarrow^{*} \gamma A \omega \Rightarrow \gamma \beta \omega
    \]
    avec \(A \to \beta \in P\), \(\gamma,\omega \in V^{*}\),
    il existe un \emph{unique} choix de production \(A \to \beta\) qui peut être appliqué
    pour réduire la \emph{poignée} \(\beta\),
    en ne regardant que :
    \begin{itemize}
        \item le \emph{préfixe viable} \(\gamma \beta\) (ce qui est déjà sur la pile),
        \item et au plus \(k\) symboles de lookahead dans \(\omega\).
    \end{itemize}

    En d'autres termes, la suite \(\beta\) est \textbf{une manche unique et non ambiguë}
    que l'analyseur peut identifier avec seulement \(k\) symboles d'anticipation.
\end{definition}

\begin{remark}
    On parlera identiquement de \emph{grammaire $LR(k)$} ou \emph{d'analyseur LR(k)}. 
    Le terme $LR(k)$ signifie : 
    \begin{itemize}
        \item \textbf{L}\emph{eft to right} : l'analyseur lit les mots de gauche à droite. 
        \item \textbf{R}\emph{ightmost derivation} : les motifs spécifiés par la grammaire sont recherchés par suffixes 
        de la chaîne lue. 
        \item $k$ : correspond au nombre de lettres non consommées considérées pour le choix de la dérivation à choisir. 
    \end{itemize}
\end{remark}

Définissons maintenant un automate capable de reconnaître de telles grammaires. 

\begin{definition}[Automate LR (bottom-up)]
    Un \emph{automate LR} est un \emph{automate à pile} qui reconnaît les préfixes viables d'une grammaire 
    hors contexte $ G = (V, \Sigma, S, P)$ où : 
    \begin{itemize}
        \item la \emph{pile} contient les symboles et états représentants un \emph{préfixe viable}. 
        \item l'entrée est la chaîne de symboles à analyser terminant par le caractère de fin \$. 
    \end{itemize}
    L'objectif est de lire la chaîne de gauche à droite en réduisant progressivement toutes poignées 
    dans le but d'obtenir l'axiome sur le sommet de la pile et d'avoir consommé toute l'entrée. 

    Formellement, l'automate peut être défini par : 
        $$ \boxed{ M = (Q, \Sigma, \Gamma, ACTION, GOTO, q_0, Z_0, F)} $$ 
    où : 
    \begin{itemize}
        \item $Q$ est l'ensembles des états de l'automate. 
        \item $ \Sigma$ représente l'alphabet terminal (lettres). 
        \item $ \Gamma = \Sigma \cup V_N$ est l'ensemble des symboles de la pile. 
        \item $q_0$ est l'état initial. 
        \item $Z_0$ est le symbole initial de la pile. 
        \item $F$ est l'ensemble des états acceptants. 
    \end{itemize}
    On définit en plus deux tables qui définissent la dynnamique de l'analyseur : 
    \begin{itemize}
        \item La table $ACTION$ qui détermine quelle action effectuer à la lecture d'une lettre : 
            \[ \boxed{ACTION : Q \times ( \Sigma \cup \{\$\}) \longrightarrow \{shift(q), reduce(A \to \beta), accept, error\} } \] 
        Elle gère les transitions sur les terminaux. 
        \item La table $GOTO$ qui détermine l'état dans lequel doit se trouver l'automate après 
        une réduction : 
            \[ \boxed{GOTO : Q \times V \longrightarrow Q} \] 
        Elle gère les transitions sur les non terminaux. 
    \end{itemize}
\end{definition}

\begin{definition}[Shift et Reduce]
    Comme dit précédement, nous n'avons que deux opérations à effectuer lors de la lecture d'un mot par une grammaire 
    $LR(k)$ (ou un automate LR) : \emph{shift} ou \emph{reduce}. Définissons les proprement : 
    \begin{enumerate}
        \item L'opération \emph{shift} consiste à déplacer le prochain symbole à lire sur la pile et aller 
        à l'état correspondant. On avance donc dans la lecture mais sans faire de réduction. 
        \item L'opération \emph{reduce} consiste, lorsque le sommet correspond à une poignée $ \beta$ pour 
        une règle de production $A \to \beta$, de : 
            \begin{itemize}
                \item Dépiler $ | \beta|$ symboles et états 
                \item Empiler le non-terminal $A$ 
                \item Aller dans l'état déterminé par la table $GOTO$
            \end{itemize}
        \item \emph{Accept} : si la pile est seulement constituée de $S'$ et que l'entrée est $\$$, alors le mot est reconnu. 
        \item \emph{Error} : si aucune opération shift/reduce n'est possible alors la configuration est invalide et le mot n'est pas reconnu. 
    \end{enumerate}
\end{definition}

Nous savons maintetant analyser un mot par un automate mais il reste un problème : comment définir les états 
de l'automate ? Pour cela, nous allons avoir besoin de définir le concept de \emph{fermeture} ou \emph{clôture}.

\begin{definition}[Fermeture]
    Soit $G' = \{V, \Sigma, P, S'\}$ une grammaire augmentée. 
    Un \emph{item $LR(0)$} est une règle de la forme : 
        \[ A \to \alpha \cdot \beta \] 
    où "$\cdot$" indique la position de la l'analyse dans la production $A \to \alpha \beta$. 
    La \emph{fermeture} d'un ensemble d'items $I$, notée $cl(I)$ est définie récursivement ainsi : 
    \begin{enumerate}
        \item $ \forall \text{ item} \in I, \text{ item}  \in cl(I)$. 
        \item Pour chaque item $A \to \alpha \cdot B \beta \in cl(I)$, et pour chaque production $ B \to \gamma \in P$, 
            alors $ B \to . \mu \in cl(I)$. 
    \end{enumerate}
\end{definition}

\begin{example}[Calcul de fermeture]
    Soit $G$ une grammaire dont les règles de production sont : 
        \[ 
            \begin{cases}
                S' \to S \\ 
                S \to AA \\ 
                A \to a 
            \end{cases}
        \] 
    On pose : 
        $$ I_0 := \{S' \to \cdots S\} $$ 
    Calculons maintenant les fermetures : 
    \begin{itemize}
        \item Le point est avant $S$ donc on ajoute $ S \to \cdot AA$ grâce à la règle $ S \to AA$. 
        \item Récursivement, le point est avant $A$ donc on ajoute $ A \to \cdot a$ grâce à la règle $A \to a$. 
        \item Le point $a \in \Sigma$ est un terminal on s'arrête donc ici. 
    \end{itemize}
    On a donc : 
        \[ cl(I_0) = \{S' \to \cdot S, S \to \cdot AA, A \to \cdot a\} \]
\end{example}

\begin{figure}[htbp]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tikzpicture}[
        node distance=3cm,
        >=Stealth,
        every state/.style={minimum size=8mm, thick, draw=black!80, fill=blue!10},
        every node/.style={font=\small}
    ]
        % états (align=center permet d'utiliser \\ dans le contenu)
        \node[state, initial, align=center] (I0) {$I_0$\\$\varepsilon$};
        \node[state, right=of I0, align=center] (I1) {$I_1$\\$n$};
        \node[state, right=of I1, align=center] (I2) {$I_2$\\$n+$};
        \node[state, right=of I2, align=center] (I3) {$I_3$\\$n+n$};

        % transitions (shifts)
        \draw[->, thick] (I0) -- node[above] {$n$} (I1);
        \draw[->, thick] (I1) -- node[above] {$+$} (I2);
        \draw[->, thick] (I2) -- node[above] {$n$} (I3);

        % handles / réductions (annotation)
        \node[below=1.2cm of I1] (red1) {Handle: $n \Rightarrow E$};
        \draw[->, dashed, red] (I1) -- (red1);

        \node[below=1.2cm of I3] (red2) {Handle: $E+E \Rightarrow E$};
        \draw[->, dashed, red] (I3) -- (red2);
    \end{tikzpicture}% 
    }
    \caption{Préfixes viables et leur correspondance avec les états de l'automate LR. 
    Chaque état $I_i$ représente un ensemble de \emph{préfixes viables} atteignables. 
    Les flèches indiquent les transitions de lecture (shift). Les lignes rouges en pointillés indiquent 
    les poignées que l'on peut réduire à partir de cet état.}
\end{figure}


\begin{example}
    Travaillons autour d'un exemple pour bien saisir toutes ces nouvelles notions. 
    Soit $G$ une grammaire dont les règles de production sont : 
        \[ 
            \begin{cases}
                S' \to S \\ 
                S \to AA \\ 
                A \to a 
            \end{cases}
        \] 
    On a ajouté un symbole $S'$ fictif pour avoir un axiome de départ unique. 
    Comme vu précédement, commençons par calculer les clôtures : 
    \begin{itemize}
        \item On calcule la clôture de $I_0$ qui donne : 
            $$ cl(I_0) = \{S' \to \cdot S, S \to \cdot AA, A \to \cdot a\}$$ 
        \item Après la lecture de $S$ nous devons passer dans un nouvel état $I_1$ d'où : 
            $$ cl(I_1) = \{S' \to S \cdot\} $$ 
        \item Après la lecture de $A$ dans $I_0$ on obtient l'ensemble $I_2$ où : 
            $$ cl(I_2) = \{S \to A \cdot A, A \to \cdot a\} $$ 
        \item Après la lecture de $A$ depuis $I_2$, on obtient $I_3$ : 
            $$ cl(I_3) = \{S \to AA \cdot\} $$ 
        \item Pour finir, après la lecture de $a$ depuis $I_0$ ou $I_2$, on a : 
            $$ cl(I_4) = \{A \to \cdot a\} $$
    \end{itemize}
    On a donc les transitions suivantes : 
        $$ \text{GOTO}(I_0, S) = I_1, \quad \text{GOTO}(I_0, A) = I_2, \quad \text{GOTO}(I_0, a) = I_5 $$ 
        $$ \text{GOTO}(I_2, A) = I_4, \quad \text{GOTO}(I_2, a) = I_4 $$ 

    \begin{center}
        \begin{minipage}{0.60\textwidth}
            On peut donc construire la table action (transitions sur les terminaux) : 
            \[ 
                \begin{array}{|c|c|c|}
                    \hline 
                    \text{ État } & a & \$ \\ 
                    \hline 
                    I_0 & \text{shift } I_4 & \text{error} \\ 
                    \hline 
                    I_1 & \text{error} & \text{accept} \\ 
                    \hline 
                    I_2 & \text{shift } I_4 & \text{error} \\ 
                    \hline 
                    I_3 & \text{ reduce } S \to AA & \text{ reduce } S \to AA \\ 
                    \hline 
                    I_4 & \text{ reduce } S \to A &  \text{ reduce } S \to A \\ 
                    \hline         
                \end{array}
            \] 
        \end{minipage}
        \hfill 
        \begin{minipage}{0.25\textwidth}
             Et la table goto sur les non terminaux : 
                \[ 
                    \begin{array}{|c|c|c|}
                        \hline 
                        \text{État} & S & A \\
                        \hline 
                        I_0 & I_1 & I_2 \\ 
                        \hline 
                        I_1 & - & - \\ 
                        \hline 
                        I_2 & - & I_3 \\ 
                        \hline 
                        I_3 & - & - \\ 
                        \hline 
                        I_4 & - & - \\ 
                        \hline  
                    \end{array}
                \] 
        \end{minipage}
    \end{center}

    Résumons le traitement de l'entrée \(w = a\,a\$\) : 

    \begin{center}
        \renewcommand{\arraystretch}{1.2}
            \resizebox{\textwidth}{!}{%
            \begin{tabular}{>{\ttfamily}c >{\ttfamily}c >{\ttfamily}c l}
                \toprule
                \textbf{Pile (état/symbole)} & \textbf{Entrée} & \textbf{Action} & \textbf{Commentaire} \\
                \midrule
                $I_0$ & a a \$ & s$I_4$ & Décalage sur \texttt{a}, empiler $I_4$ \\
                $I_0$ a $I_4$ & a \$ & r($A \to a$) & Réduction : dépiler a et $I_4$, empiler A puis GOTO[$I_0$,A]=$I_2$ \\
                $I_0$ A $I_2$ & a \$ & s$I_4$ & Décalage sur \texttt{a}, empiler $I_4$ \\
                $I_0$ A $I_2$ a $I_4$ & \$ & r($A \to a$) & Réduction : dépiler a et $I_4$, empiler A puis GOTO[$I_2$,A]=$I_3$ \\
                $I_0$ A $I_2$ A $I_3$ & \$ & r($S \to A A$) & Réduction : dépiler deux $A$ et états, empiler $S$ puis GOTO[$I_0$,S]=$I_1$ \\
                $I_0$ S $I_1$ & \$ & accept & Mot reconnu, analyse terminée \\
                \bottomrule
            \end{tabular}%
            }
    \end{center}
\end{example}
